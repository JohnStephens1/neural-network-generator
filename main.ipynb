{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from random import sample\n",
    "from functions.data_preparation import *\n",
    "from functions.quick_maths import *\n",
    "from functions.util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### neural network building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "\tparameters = {}\n",
    "\n",
    "\tfor l in range(1, len(layer_dims)):\n",
    "\t\tparameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "\t\tparameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "\treturn parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(W, b, A_prev):\n",
    "\tZ = np.dot(W, A_prev) + b\n",
    "\n",
    "\tWbA_prev = (W, b, A_prev)\n",
    "\n",
    "\treturn Z, WbA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linear_activation_forward(W, b, A, activation):\n",
    "\tZ, WbA = linear_forward(W, b, A)\n",
    "\n",
    "\tA = activation(Z)\n",
    "\n",
    "\treturn A, (WbA, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X, parameters, activation_functions, L):\n",
    "\tcaches = []\n",
    "\tA = X\n",
    "\n",
    "\tfor l in range(1, L):\n",
    "\t\tA, WbA_Z = linear_activation_forward(parameters[f\"W{l}\"], parameters[f\"b{l}\"], A, activation_functions[l - 1])\n",
    "\t\tcaches.append(WbA_Z)\n",
    "\n",
    "\ty_hat = A  # the final value of A is equivalent to y_hat, it's more logical to return the proper term\n",
    "\treturn y_hat, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache, m):\n",
    "\tW, b, A_prev = cache\n",
    "\n",
    "\tdW = 1 / m * np.dot(dZ, A_prev.T)\n",
    "\tdb = 1 / m * np.sum(dZ, axis=1, keepdims=True)\n",
    "\tdA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "\treturn dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, caches, d_activation, m):\n",
    "\tlinear_cache, activation_cache = caches\n",
    "\n",
    "\tdZ = dA * d_activation(activation_cache)\n",
    "\tdW, db, dA_prev = linear_backward(dZ, linear_cache, m)\n",
    "\n",
    "\treturn dW, db, dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def back_propagation(Y_hat, Y, caches, d_act_functions, m):\n",
    "\tgrads = {}\n",
    "\tL = len(caches)  # n_activated_layers\n",
    "\n",
    "\tgrads[f\"dA{L}\"] = -(np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
    "\n",
    "\tfor l in reversed(range(1, L + 1)):\n",
    "\t\tgrads[f\"dW{l}\"], grads[f\"db{l}\"], grads[f\"dA{l - 1}\"] = linear_activation_backward(grads[f\"dA{l}\"],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   caches[l - 1],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   d_act_functions[l - 1], m)\n",
    "\n",
    "\treturn grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, L, alpha, reg_term):\n",
    "\tparams = parameters.copy()\n",
    "\n",
    "\tfor l in range(1, L):\n",
    "\t\tparams[f\"W{l}\"] -= (alpha * (grads[f\"dW{l}\"] + reg_term[l - 1]))\n",
    "\t\tparams[f\"b{l}\"] -= alpha * (grads[f\"db{l}\"])\n",
    "\n",
    "\treturn params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### debug helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def forward_prop_and_cost(X, Y, parameters, act_functions, L, m):\n",
    "\tY_hat, _ = forward_propagation(X, parameters, act_functions, L)\n",
    "\tcost = cross_entropy_cost(Y_hat, Y, m)\n",
    "\n",
    "\treturn cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_nudged_cost(X, Y, theta, i, layer_dims, act_functions, L, m, epsilon):\n",
    "\ttheta[i] += epsilon\n",
    "\tcost = forward_prop_and_cost(X, Y, vector_to_dict(theta, \"W\", \"b\", layer_dims), act_functions, L, m)\n",
    "\treturn cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def check_gradients(X, Y, parameters, grads, layer_dims, act_functions, L, m, epsilon=10e-7):\n",
    "\ttheta = dict_to_vector(parameters, 'W', 'b', L)\n",
    "\td_theta = dict_to_vector(grads, 'dW', 'db', L)\n",
    "\n",
    "\td_theta_approx = np.zeros_like(theta)\n",
    "\n",
    "\tfor i in range(len(theta)):\n",
    "\t\tcost_plus = get_nudged_cost(X, Y, theta.copy(), i, layer_dims, act_functions, L, m, epsilon)\n",
    "\t\tcost_minus = get_nudged_cost(X, Y, theta.copy(), i, layer_dims, act_functions, L, m, -epsilon)\n",
    "\n",
    "\t\td_theta_approx[i] = (cost_plus - cost_minus) / (2 * epsilon)\n",
    "\n",
    "\tremainder = L2_norm(d_theta_approx - d_theta) / (L2_norm(d_theta_approx) + L2_norm(d_theta))\n",
    "\n",
    "\tif remainder > 10e-5:\n",
    "\t\tprint(\n",
    "\t\t\tf\"Warning!\\nError is unusually high ({remainder}), there is most likely an error in gradient calculation!\")\n",
    "\telse:\n",
    "\t\t# print(f\"remainder is {remainder}! Gradient implementation should be correct!\")\n",
    "\t\t[print(\n",
    "\t\t\tf\"\\033[{x}m`!;<>;`!`!;<>;`!remainder is {remainder}! Gradient implementation should be correct!!`!;<>;`!`!;<>;`!\\033[0m\")\n",
    "\t\t for x in range(0, 130)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(1) + \"\\033[0m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### general utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pipeline(X, Y, m_axis, m_reduced, hyper_parameters):\n",
    "\tX, Y = get_aligned_dfs(X, Y, m_axis)\n",
    "\n",
    "\tif m_reduced != 0:\n",
    "\t\tX, Y = get_reduced_dataframes(X, Y, m_reduced, m_axis)\n",
    "\n",
    "\t# slightly verbose for clarity\n",
    "\tlearned_parameters, computed_costs = train_model(X, Y, m_axis, **hyper_parameters)\n",
    "\n",
    "\treturn learned_parameters, computed_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def alpha_decay(alpha, count_decreased_alpha, consec_cost_increases):\n",
    "\tif consec_cost_increases == 3:\n",
    "\t\tfancy_print(f'Cost is increasing. Reducing alpha from {alpha} to {alpha / 3}', 'yellow')\n",
    "\t\talpha /= 3\n",
    "\t\tcount_decreased_alpha += 1\n",
    "\treturn alpha, count_decreased_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def manage_costs(cost, last_cost, costs, consec_cost_increases, i, num_iter):\n",
    "\tif i % 5 == 0: costs.append((i, cost))\n",
    "\n",
    "\tif cost < last_cost or last_cost == 0:\n",
    "\t\tconsec_cost_increases = 0\n",
    "\telse:\n",
    "\t\tconsec_cost_increases += 1\n",
    "\n",
    "\tif i % 20 == 0 or i == num_iter - 1:\n",
    "\t\tprint(f\"Cost at {i}: {cost}\")\n",
    "\n",
    "\treturn consec_cost_increases, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def update_best_parameters(parameters, cost, last_cost):\n",
    "\tif cost < last_cost:\n",
    "\t\treturn parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### THE HEART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(X, Y, m_axis, layer_dims, act_list, act_dict, num_iterations, alpha, lambd):\n",
    "\tnp.random.seed(1)\n",
    "\n",
    "\tact_functions = get_act_functions(act_list, act_dict)\n",
    "\td_act_functions = get_act_functions(get_d_strings(act_list), act_dict)\n",
    "\n",
    "\tconsec_cost_increases = 0\n",
    "\tcount_decreased_alpha = 0\n",
    "\n",
    "\t# starting costs with an ambiguous large value to avoid an unnecessary if statement on ever iteration\n",
    "\tcosts = []\n",
    "\tlast_cost = 0\n",
    "\n",
    "\tm = Y.shape[m_axis]\n",
    "\tL = len(layer_dims)\n",
    "\treg_term = [0 for _ in range(L)]\n",
    "\tlayer_dims[0] = X.shape[1 - m_axis]\n",
    "\n",
    "\tparameters = initialize_parameters(layer_dims)\n",
    "\tbest_parameters = parameters\n",
    "\n",
    "\tfor i in range(num_iterations):\n",
    "\t\tY_hat, caches = forward_propagation(X, parameters, act_functions, L)\n",
    "\n",
    "\t\tif lambd == 0:\n",
    "\t\t\tcost = cross_entropy_cost(Y_hat, Y, m)\n",
    "\t\telse:\n",
    "\t\t\tweights = get_weights(parameters, L)\n",
    "\t\t\tcost = L2_cross_entropy_cost(Y_hat, Y, lambd, weights, m)\n",
    "\t\t\treg_term = d_L2_regularization(lambd, m, weights)\n",
    "\n",
    "\t\tgrads = back_propagation(Y_hat, Y, caches, d_act_functions, m)\n",
    "\n",
    "\t\tbest_parameters = update_best_parameters(parameters, cost, last_cost)\n",
    "\n",
    "\t\t# check_gradients(X, Y, parameters, grads, layer_dims, act_functions, L)\n",
    "\n",
    "\t\tparameters = update_parameters(parameters, grads, L, alpha, reg_term)\n",
    "\n",
    "\t\tvector = dict_to_vector(parameters, 'W', 'b', L)\n",
    "\t\tnp.testing.assert_equal(vector, dict_to_vector(parameters, 'W', 'b', L),\n",
    "\t\t\t\t\t\t\t\tvector_to_dict(vector, 'W', 'b', layer_dims), verbose=True)\n",
    "\n",
    "\t\tconsec_cost_increases = manage_costs(cost, last_cost, costs, consec_cost_increases, i, num_iterations)\n",
    "\n",
    "\t\talpha, count_decreased_alpha = alpha_decay(alpha, count_decreased_alpha, consec_cost_increases)\n",
    "\n",
    "\t\tif count_decreased_alpha == 7:\n",
    "\t\t\tprint(\"Cost increasing, stopping training early.\\nThe best parameters learned were saved and returned.\")\n",
    "\t\t\treturn best_parameters, costs\n",
    "\n",
    "\tfancy_print(f\"`!;<>;`!\\nTraining finished successfully {num_iterations}!\", 'green')\n",
    "\treturn best_parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# global variables\n",
    "np.random.seed(1)\n",
    "sns.set_style(\"darkgrid\")\n",
    "activation_dict = {\"relu\": relu, \"d_relu\": d_relu, \"sigmoid\": sigmoid, \"d_sigmoid\": d_sigmoid, \"leaky_relu\": leaky_relu,\n",
    "\t\t\t\t   \"d_leaky_relu\": d_leaky_relu}\n",
    "m_axis = 1\n",
    "\n",
    "# adaptive global variables\n",
    "m_reduced = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = prepare_dataframes(*load_data(), m_axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hyper_parameters = {\n",
    "\t'alpha': 0.3,\n",
    "\t'lambd': 0,\n",
    "\t'num_iterations': 3000,\n",
    "\t'layer_dims': [0, 3, 2, 1],  # first entry gets updated, fret not\n",
    "\t'act_list': ['relu', 'relu', 'sigmoid'],\n",
    "\t'act_dict': activation_dict\n",
    "}\n",
    "\n",
    "gradient_checking_parameters = {\n",
    "\t'X': X_train,\n",
    "\t'Y': Y_train,\n",
    "\t'm_axis': m_axis,\n",
    "\t'm_reduced': 5,\n",
    "\t'hyper_parameters': {\n",
    "\t\t'alpha': 0.1,\n",
    "\t\t'lambd': 0,\n",
    "\t\t'num_iterations': 1,\n",
    "\t\t'layer_dims': [0, 3, 2, 1],\n",
    "\t\t'act_list': ['relu', 'relu', 'sigmoid'],\n",
    "\t\t'act_dict': activation_dict}\n",
    "}\n",
    "\n",
    "# learned_parameters, computed_costs = pipeline(X_train, Y_train, m_axis, m_reduced, hyper_parameters)\n",
    "# pipeline(**gradient_checking_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learned_parameters, computed_costs = pipeline(X_train, Y_train, m_axis, m_reduced, hyper_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# p, c = pipeline(**gradient_checking_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### construction site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Stats:\n",
    "\tdef __init__(self, tp, tn, fp, fn):\n",
    "\t\tself.tp =\n",
    "\t\tself.tn = get_true_negatives()\n",
    "\t\tself.fp =\n",
    "\t\tself.fn = fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_stats(Y, predictions):\n",
    "\ttrue_positives = np.sum(Y == 1 == predictions)\n",
    "\treturn p, n, tp, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_true_positives(Y, predictions):\n",
    "\treturn np.sum(np.equal(np.equal(Y, 1), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_true_negatives(Y, predictions):\n",
    "\treturn np.sum(np.equal(np.equal(Y, 0), predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_predictions(Y_hat, threshold=0.5):\n",
    "\tpredictions = np.ones_like(Y_hat)\n",
    "\tpredictions[Y_hat < threshold] = 0\n",
    "\treturn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict(X, params, activation_list, activation_dict, L):\n",
    "\tact_functions = get_act_functions(activation_list, activation_dict)\n",
    "\n",
    "\tY_hat, _ = forward_propagation(X, params, act_functions, L)\n",
    "\n",
    "\treturn Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y = np.array([1, 0, 1, 1, 0]).reshape([-1, 1])\n",
    "Y_hat = np.array([0.8, 0.4, 0.1, 0.9, 0.7]).reshape([-1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(np.equal(Y, get_predictions(Y_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(Y, get_predictions(Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "get_true_positives(Y, get_predictions(Y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = get_predictions(Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "get_true_positives(Y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.array([1, 0, 1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def accuracy():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_hat = predict(X_val, learned_parameters, activation_list, activation_dict, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_hat).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predictions = get_predictions(Y_hat, Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predict(X_val, Y_val, learned_parameters, activation_list, activation_dict, L).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calculate_metrics(learned_parameters, m_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def save_parameters(params):\n",
    "\tnp.save('best_parameters', params)\n",
    "\n",
    "# save_parameters(learned_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Y_train[:, :100].T).describe()\n",
    "# TODO\n",
    "# SHUFFLE DATAFRAME PROPERLY, ITS FKING SORTED xd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_metrics(params, X, Y):\n",
    "\n",
    "# ADD ROC CURVE;  PRECISION/RECALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.lineplot(x=[x[0] for x in computed_costs], y=[x[1] for x in computed_costs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
